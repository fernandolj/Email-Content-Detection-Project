ULMFiT pretrains a Language Model (LM) on a large general-domain corpus and fine-tunes (FT) it on the target task using novel techniques because it allows the model to work across various sizes, number, and label types, it uses a single architecture and training process, it requires no custom features or preprocessing, and it does not require additional in-domain documents or labels allowing the model to be universal in a sense. ULMFiT has three phases: AWD-LSTM, unsupervised pre-training on a large corpus, which is expensive and slow but only happens once. “Semi-supervised” LMFT on task related data and classifier FT on potentially very small task-specific dataset, this phase is just designed to fine-tune the model on other datasets and a different and ont learn an entirely new LM. It implements Discriminative FT (DISCR) which is simply using different learning rates for different layers, authors determined a factor of 2.6 for successive layers was good, and Slanted Triangular Learning Rates (STLR) in order to circumvent many of the early steps and errors of introducing the model to a new distribution. The third phase is classifier FT, where the authors implemented computer vision techniques, two fully connected layers with batch norm and dropout, with ReLU and softmax as a classification head, alongside max- and mean-pooling. It also uses DISCR and STLR with an additional task of adding gradual unfreezing of layers to prevent catastrophic forgetting from overly FT and slow convergence from being too cautious. They also implement an augmented version of basic BackPropagation Through Time (BPTT), a gradient-based technique where the recurrent model is initialized to the final state of the previous batch, alongside variable-length BPTT, and bidirectional FT. They used 6 widely studied datasets with varying numbers of documents, of varying length documents; TREC-6 with 5.5k, IMDb with 25k, Yelp-bi with 560k, Yelp-full with 650k, AG with 120k, and DBpedia with 560k samples, as instances of sentiment analysis, question classification, and topic classification. IMDb and both Yelp db’s underwent sentiment analysis, TREC-6 underwent question classification, and AG and DBpedia underwent topic classification. The AWD_LSTM LM was used with an embedding size of 400, 3 layers, 1150 hidden activations per layer, and a BPTT batch size of 70, a dropout of 0.4 layers, 0.3 RNN layers, 0.4 to input embedding layers, and weight dropout of 0.5 to the RNN hidden-to-hidden matrix for the hyperparamaters. They used the same preprocessing techniques as previous work.
Many of the tools the authors used are quite simple in regards to other fine-tuning models without the need for any custom or very difficult algorithms. We can apply many of the same methods to our text classification model as they are fairly standard models all working together in unison, ie. preprocessing the data, using discriminative fine tuning as well as slanted triangle learning rate and gradually unfreezing layers.
